{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists, expanduser\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 120\n",
    "SEED = 2018\n",
    "data_dir = 'data/'\n",
    "labels = pd.read_csv(join(data_dir, 'labels.csv'))\n",
    "sample_submission = pd.read_csv(join(data_dir, 'sample_submission.csv'))\n",
    "print(\"Number of samples: {}\".format(len(listdir(join(data_dir, 'train')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels.groupby(\"breed\").count().sort_values(\"id\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels.groupby(\"breed\").count().sort_values(\"id\", ascending=False).tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_enc = LabelEncoder()\n",
    "np.random.seed(seed=SEED)\n",
    "rnd = np.random.random(len(labels))\n",
    "train_idx = rnd < 0.9\n",
    "valid_idx = rnd >= 0.9\n",
    "y_train = label_enc.fit_transform(labels[\"breed\"].values)\n",
    "ytr = y_train[train_idx]\n",
    "yv = y_train[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications import xception\n",
    "\n",
    "def read_img(img_id, train_or_test, size):\n",
    "    \"\"\"Read and resize image.\n",
    "    # Arguments\n",
    "        img_id: string\n",
    "        train_or_test: string 'train' or 'test'.\n",
    "        size: resize the original image.\n",
    "    # Returns\n",
    "        Image as numpy array.\n",
    "    \"\"\"\n",
    "    img = image.load_img(join(data_dir, train_or_test, '{}.jpg'.format(img_id)), target_size=size)\n",
    "    return image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_img(img, label, ax):\n",
    "    ax.imshow(img / 255.)\n",
    "    ax.text(10, 200, 'LABEL: {}'.format(label), color='k', backgroundcolor='w', alpha=0.8)\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "fig = plt.figure(1, figsize=(16, 16))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(3, 3), axes_pad=0.05)\n",
    "for i, idx in enumerate(np.random.choice(list(range(len(labels))), size=9)):\n",
    "    ax = grid[i]\n",
    "    show_img(read_img(labels['id'][idx], \"train\", (299, 299)), label=labels['breed'][idx], ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "INPUT_SIZE = 299\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "x_train = np.zeros((train_idx.sum(), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "x_valid = np.zeros((valid_idx.sum(), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "train_i = 0\n",
    "valid_i = 0\n",
    "for i, img_id in tqdm(enumerate(labels['id'])):\n",
    "    img = read_img(img_id, 'train', (INPUT_SIZE, INPUT_SIZE))\n",
    "    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "    if train_idx[i]:\n",
    "        x_train[train_i] = x\n",
    "        train_i += 1\n",
    "    elif valid_idx[i]:\n",
    "        x_valid[valid_i] = x\n",
    "        valid_i += 1\n",
    "print('Train Images shape: {} size: {:,}'.format(x_train.shape, x_train.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=45,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.25,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.models import Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = xception.Xception(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = BatchNormalization()(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# and a logistic layer -- let's say we have NUM_CLASSES classes\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# checkpoints\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "STAMP = \"{}_dog_breed_model\".format(datetime.date.today().strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "bst_model_path = \"models/{}.h5\".format(STAMP)\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True)\n",
    "\n",
    "# Authors suggest setting step_size = (2-8) x (training iterations in epoch)\n",
    "step_size = 2000\n",
    "clr = CyclicLR(base_lr=0.0001,\n",
    "               max_lr=0.001,\n",
    "               step_size=step_size,\n",
    "               mode='exp_range',\n",
    "               gamma=0.99994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional Xception layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "hist = model.fit_generator(train_datagen.flow(x_train, ytr, batch_size=BATCH_SIZE),\n",
    "                           steps_per_epoch=train_idx.sum() // BATCH_SIZE,\n",
    "                           epochs=50, callbacks=[early_stopping, model_checkpoint, clr],\n",
    "                           validation_data=test_datagen.flow(x_valid, yv, batch_size=BATCH_SIZE),\n",
    "                           validation_steps=valid_idx.sum() // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with original approach:\n",
    "\n",
    "Epoch 1/50\n",
    "573/573 [==============================] - 188s 329ms/step - loss: 2.6888 - acc: 0.4172 - val_loss: 0.5156 - val_acc: 0.8356\n",
    "Epoch 2/50\n",
    "573/573 [==============================] - 186s 325ms/step - loss: 1.1958 - acc: 0.6704 - val_loss: 0.4518 - val_acc: 0.8423\n",
    "Epoch 3/50\n",
    "573/573 [==============================] - 186s 325ms/step - loss: 1.2034 - acc: 0.6921 - val_loss: 0.4862 - val_acc: 0.8519\n",
    "Epoch 4/50\n",
    "573/573 [==============================] - 185s 323ms/step - loss: 1.2603 - acc: 0.6965 - val_loss: 0.4370 - val_acc: 0.8673\n",
    "Epoch 5/50\n",
    "573/573 [==============================] - 186s 324ms/step - loss: 1.2512 - acc: 0.7045 - val_loss: 0.4166 - val_acc: 0.8644\n",
    "Epoch 6/50\n",
    " 98/573 [====>.........................] - ETA: 2:21 - loss: 1.0983 - acc: 0.7392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_py36",
   "language": "python",
   "name": "tensorflow_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
